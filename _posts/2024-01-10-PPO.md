---
title: 석나가는 PPO (1) 이론편
author: kotmul
date: 2024-01-02 03:54:00 +0900
categories: [RL]
tags: [RL]
pin: true
math: true
render_with_liquid: false
comments: True
---

# Proximal Policy Optimization
## 01. Preliminaries
*****
TRPO는 monotonic improvement를 보장하여 high preformance를 이뤄냈다. 해당 알고리즘의 주요 아이디어는 다음과 같다.
- Minorization - Maximization Algorithm
- Trust Region


이 때, Trust Region은 monotonic improvement를 이뤄내는데 큰 역할을 수행한다.

그럼에도 불구하고, TRPO는 Implementation과 Computation이 복잡하다는 단점을 갖고 있다.

이러한 단점이 발생하는 이유에 대해 알아보자.

### 01.01. Why do these shortcomings occur?
TRPO는 Total Retrun을 Maximize하기 위해 다양한 approximation을 적용한다. 이를 통해 최종적으로 optimization하는 식은 아래와 같다.

$$\max_{\theta}L^{\mathrm{TRPO}}(\theta) = \mathbb{E}\Big[\frac{\pi_{\theta}(a|s)}{\pi_{\theta_{old}}(a|s)}A_{\pi_{old}}(s, a)\Big] \quad \mathit{subject \;to} \quad \mathbb{E}\Big[D_{KL}(\pi_{old}(\cdot|s)||\pi(\cdot|s))\Big]\leq\delta$$

\\
해당 식은 Taylor Series를 통해 approximation하여 optimization이 수행된다. 이 때, KL-Constraint는 이계도함수로 approximation된다.

KL-Constraint는 Trust Region을 확보하여 monotonic improvement를 가져온다는 어마한 장점을 갖고있지만, approximation시 발생하는 이계도함수는 Fisher information matrix($\mathrm{FIM}$)인 $\mathrm{H}$를 구해야 한다는 한계점이 있다. 이는 앞서 언급한 TRPO의 단점인 Implementation과 Computation이 복잡함의 주요 원인이다.

## 02. Proximal Policy Optimization
*****
Proximal Policy Optimization(PPO)는 TRPO의 단점을 해결하고자 KL-Constraint를 배제한다. 그로 인해 발생하는 문제점들을 해결하기 위하여, PPO introduces Clipped surrogate objective function by clipping a policy ratio $\;r(\theta)=\frac{\pi_{\theta}(a|s)}{\pi_{\theta_{old}}(a|s)}$

$$\max_{\theta}L^{\mathrm{CLIP}}(\theta) = \mathbb{E}\Big[\min(r(\theta)A_{\theta_{old}}(s, a), \mathrm{clip(r(\theta), 1-\epsilon, 1+\epsilon)}A_{\theta_{old}}(s, a))\Big]$$

\\
Clipping method는 1st order stochastic gradient ascent를 사용하여 Compuation을 간단하게 만든다.

쓰기 귀찮지만, Clipping method는 MM algorithm을 적용하기 위한 3가지 조건 또한 만족한다.

### 02.01. Practical Implementation